{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maab2198/nlp2021/blob/labs/lab6/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3e-r7QcEf-X",
        "outputId": "bd71930e-9896-4913-cc14-3370b1365fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 457 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGucUNwXEf-c",
        "outputId": "a11762ce-8ab8-4e40-c50c-130c7f319f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            ", Dave, watched, as, the, forest, burned, up, on, the, hill, ,, \n",
            ", only, a, few, miles, from, his, house, ., The, car, had, \n",
            ", been, hastily, packed, and, Marta, was, inside, trying, to, round, \n",
            ", up, the, last, of, the, pets, ., \", Where, could, she, be, ?, \", he, wondered, \n",
            ", as, he, continued, to, wait, for, Marta, to, appear, with, the, pets, ., \n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"\n",
        "Dave watched as the forest burned up on the hill,\n",
        "only a few miles from his house. The car had\n",
        "been hastily packed and Marta was inside trying to round\n",
        "up the last of the pets. \"Where could she be?\" he wondered\n",
        "as he continued to wait for Marta to appear with the pets.\n",
        "\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "token_list = [token for token in doc]\n",
        "\n",
        "print(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtdMo5OTEf-f",
        "outputId": "e11e5879-b318-400e-be3b-6e8486a191c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            ", Dave, watched, forest, burned, hill, ,, \n",
            ", miles, house, ., car, \n",
            ", hastily, packed, Marta, inside, trying, round, \n",
            ", pets, ., \", ?, \", wondered, \n",
            ", continued, wait, Marta, appear, pets, ., \n",
            "]\n"
          ]
        }
      ],
      "source": [
        "filtered_tokens = [token for token in doc if not token.is_stop]\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHcpVoHeEf-i",
        "outputId": "850019c6-97cf-44b6-ba44-abea880fcff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Token: \\n, lemma: \\n', 'Token: Dave, lemma: Dave', 'Token: watched, lemma: watch', 'Token: forest, lemma: forest', 'Token: burned, lemma: burn', 'Token: hill, lemma: hill', 'Token: ,, lemma: ,', 'Token: \\n, lemma: \\n', 'Token: miles, lemma: mile', 'Token: house, lemma: house', 'Token: ., lemma: .', 'Token: car, lemma: car', 'Token: \\n, lemma: \\n', 'Token: hastily, lemma: hastily', 'Token: packed, lemma: pack', 'Token: Marta, lemma: Marta', 'Token: inside, lemma: inside', 'Token: trying, lemma: try', 'Token: round, lemma: round', 'Token: \\n, lemma: \\n', 'Token: pets, lemma: pet', 'Token: ., lemma: .', 'Token: \", lemma: \"', 'Token: ?, lemma: ?', 'Token: \", lemma: \"', 'Token: wondered, lemma: wonder', 'Token: \\n, lemma: \\n', 'Token: continued, lemma: continue', 'Token: wait, lemma: wait', 'Token: Marta, lemma: Marta', 'Token: appear, lemma: appear', 'Token: pets, lemma: pet', 'Token: ., lemma: .', 'Token: \\n, lemma: \\n']\n"
          ]
        }
      ],
      "source": [
        "lemmas = [\n",
        "    f\"Token: {token}, lemma: {token.lemma_}\"\n",
        "    for token in filtered_tokens\n",
        "]\n",
        "\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvSUw-heEf-l",
        "outputId": "629a8751-817f-4c99-b145-14a0b3c8bc14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.6193167e+00, -2.7117019e+00, -6.8552375e-01,  2.6652899e+00,\n",
              "        4.5226312e+00,  2.8338575e+00,  6.1740106e-01,  9.5401168e-01,\n",
              "        2.6201737e+00,  2.5994289e+00,  5.9061027e+00, -1.7552420e-01,\n",
              "       -8.7880111e-01,  4.8553795e-03, -1.7236035e+00, -1.7494547e+00,\n",
              "       -1.0313329e+00,  1.6518956e-01,  5.3024960e-01, -3.2018152e-01,\n",
              "       -2.6411371e+00, -2.4750671e+00, -5.0014794e-01, -3.3213449e+00,\n",
              "       -5.3300351e-01,  2.3968523e+00,  1.5485952e+00, -2.2231889e+00,\n",
              "       -1.2597762e+00, -5.6858027e-01, -9.4768405e-02, -1.3759263e+00,\n",
              "       -1.0165324e+00,  5.6860483e-01,  2.6817162e+00, -3.7418640e+00,\n",
              "        2.7644300e+00, -1.9967061e+00, -2.9627855e+00, -1.0863459e-01,\n",
              "        2.7437925e+00,  2.5450244e+00,  1.6124392e+00, -3.3037057e+00,\n",
              "       -2.4419413e+00,  9.5868981e-01,  1.1957375e+00, -1.2429583e+00,\n",
              "       -1.2961357e+00,  2.8916957e+00, -2.8091950e+00, -3.1826324e+00,\n",
              "       -2.4809690e+00, -2.5254309e-01, -2.0454383e+00,  3.0948038e+00,\n",
              "        2.3146892e+00,  3.1973858e+00, -1.0000327e+00, -1.8173008e+00,\n",
              "       -1.8257004e-01, -1.3169163e-01, -1.6473753e+00, -3.0071383e+00,\n",
              "        5.8041401e+00, -2.8103060e-01,  1.7717384e-01, -2.2952008e+00,\n",
              "       -1.3665587e+00,  4.3192568e-01,  1.5252322e+00,  1.0701846e+00,\n",
              "        4.5825213e-01, -1.7640622e+00,  1.0941651e+00, -3.9024787e+00,\n",
              "        2.3232937e-02, -4.5654988e-01, -2.3950067e+00,  1.0093416e+00,\n",
              "        3.8597989e+00, -1.0375429e+00, -2.4387794e+00, -1.1583717e+00,\n",
              "        3.8506849e+00,  3.5678258e+00, -2.9660366e+00, -3.1863713e+00,\n",
              "        4.5841753e-02,  8.6235547e-01, -1.8335643e+00, -1.6974587e+00,\n",
              "       -1.1221293e+00,  2.0094342e+00,  3.8732340e+00,  2.9344873e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "filtered_tokens[1].vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CbqlfhHJEf-n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(os.path.relpath('./'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GwGKAiVfEf-o"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "fname = 'aclImdb_v1.tar.gz'\n",
        "with tarfile.open(fname, \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tkTWtlLUEf-q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random                                  \n",
        "\n",
        "def load_training_data(\n",
        "    data_directory: str = \"aclImdb/train\",\n",
        "    split: float = 0.8,\n",
        "    limit: int = 0\n",
        ") -> tuple:\n",
        "\n",
        "    reviews = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        labeled_directory = f\"{data_directory}/{label}\"\n",
        "        for review in os.listdir(labeled_directory):\n",
        "            if review.endswith(\".txt\"):\n",
        "                with open(f\"{labeled_directory}/{review}\") as f:\n",
        "                    text = f.read()\n",
        "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
        "                    if text.strip():\n",
        "                        spacy_label = {\n",
        "                            \"cats\": {\n",
        "                                \"pos\": \"pos\" == label,\n",
        "                                \"neg\": \"neg\" == label}\n",
        "                        }\n",
        "                        reviews.append((text, spacy_label))\n",
        "    random.shuffle(reviews)                    \n",
        "\n",
        "    if limit:                                  \n",
        "        reviews = reviews[:limit]              \n",
        "    split = int(len(reviews) * split)          \n",
        "    return reviews[:split], reviews[split:]    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOUn2rwXEf-r",
        "outputId": "efeca1e3-fd3a-4c37-bcac-4d97fe7dca65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"In Hazzard County, Georgia, cousins Bo and Luke Duke (Scott, Knoxville) and their cousin Daisy Duke (Jessica Simpson) run moonshine made by their Uncle Jesse (Willie Nelson) while avoiding the local authority, Boss Hog (Burt Reynolds). Their problems with the Boss are only beginning as they learn he's been plotting to strip mine the town for valuable ores found below it.\\n\\n\\n\\nI have never seen the TV show and after watching the movie, I'm not going to start any time soon. I like stupid comedies but this one didn't offer many laughs. It was a pretty dull picture with the first hour being really hard to sit through. The second part was a little better but this film was still a missed opportunity. The film focused on Bo and Luke way too much. The characters in general weren't very interesting and the actors portraying them didn't do a very good job.\\n\\n\\n\\nThe acting wasn't very good. I wasn't expecting it to be good in the first place but none of the leads were very funny. Seann William Scott and Johnny Knoxville both give below average performances. The latter was pretty good as Stifler but he tries way too hard here. The latter just seems to be looking for a paycheck and nothing else. Jessica Simpson isn't known for her acting nor is she really known for her singing. She's famous for having her own reality show and for saying really dumb things. She is pretty but she's a weak actor. It doesn't matter though because she doesn't really appear in the movie and the character she plays isn't complex or anything. Willie Nelson also has a minor role and he doesn't do anything special.\\n\\n\\n\\nThe screenplay was written by John O'Brien and he made two films prior to the Dukes of Hazzard. The first one was Cradle to the Grave, which was okay. The second one was Starsky and Hutch which was pretty funny. He doesn't do a good job here though as the story is a mess. He also forgot to add jokes and a few other things that would have made this film work better. The movie is also pretty long for a comedy. Okay, 106 minutes isn't exactly long but it feels so much longer because there's very little humor in the first hour. I think comedies should be kept short or else they have to find a lot of material to cover the entire running time. The Dukes of Hazzard barely has enough funny gags to keep it going for thirty minutes let alone 106 minutes. The car sequences were average and they don't save an already troubled film. In the end, Dukes of Hazzard may appeal to a few people but most people will probably find it dull and it's better if you just skip it. Rating 4/10\",\n",
              " {'cats': {'neg': True, 'pos': False}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "load_training_data(\n",
        "    data_directory = \"aclImdb/train\",\n",
        "    split = 0.8,\n",
        "    limit = 0)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3lzZpD8uEf-w"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(tokenizer, textcat, test_data: list) -> dict:\n",
        "    reviews, labels = zip(*test_data)\n",
        "    reviews = (tokenizer(review) for review in reviews)\n",
        "\n",
        "    TP, FP, TN, FN = 1e-8, 0, 0, 0\n",
        "    for i, review in enumerate(textcat.pipe(reviews)):\n",
        "        true_label = labels[i]['cats']\n",
        "        score_pos = review.cats['pos'] \n",
        "        if true_label['pos']:\n",
        "            if score_pos >= 0.5:\n",
        "                TP += 1\n",
        "            else:\n",
        "                FN += 1\n",
        "        else:\n",
        "            if score_pos >= 0.5:\n",
        "                FP += 1\n",
        "            else:\n",
        "                TN += 1    \n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "    f_score = 2 * precision * recall / (precision + recall)\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "AekDy3v6Ef-x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "def train_model(\n",
        "    training_data: list,\n",
        "    test_data: list,\n",
        "    iterations: int = 20) -> None:\n",
        " # Build pipeline\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textcat\" not in nlp.pipe_names:\n",
        "        textcat = nlp.create_pipe(\n",
        "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
        "        )\n",
        "        nlp.add_pipe(textcat, last=True)\n",
        "    else:\n",
        "        textcat = nlp.get_pipe(\"textcat\")\n",
        "\n",
        "    textcat.add_label(\"pos\")\n",
        "    textcat.add_label(\"neg\")\n",
        "\n",
        "     # Train only textcat\n",
        "    training_excluded_pipes = [\n",
        "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
        "    ]\n",
        "    with nlp.disable_pipes(training_excluded_pipes):\n",
        "        optimizer = nlp.begin_training()\n",
        "        # Training loop\n",
        "  \n",
        "        print(\"Loss\\t\\tPrec.\\tRec.\\tF-score\")          \n",
        "        batch_sizes = compounding(\n",
        "            4.0, 32.0, 1.001\n",
        "        )  # A generator that yields infinite series of input numbers\n",
        "        for i in range(iterations):\n",
        "            loss = {}\n",
        "            random.shuffle(training_data)\n",
        "            batches = minibatch(training_data, size=batch_sizes)\n",
        "            for batch in batches:\n",
        "                text, labels = zip(*batch)\n",
        "                nlp.update(\n",
        "                    text,\n",
        "                    labels,\n",
        "                    drop=0.2,\n",
        "                    sgd=optimizer,\n",
        "                    losses=loss\n",
        "                )\n",
        "            with textcat.model.use_params(optimizer.averages):\n",
        "                evaluation_results = evaluate_model(   \n",
        "                    tokenizer=nlp.tokenizer,           \n",
        "                    textcat=textcat,                   \n",
        "                    test_data=test_data                \n",
        "                )                                      \n",
        "                print(f\"{loss['textcat']:9.6f}\\t\\\n",
        "{evaluation_results['precision']:.3f}\\t\\\n",
        "{evaluation_results['recall']:.3f}\\t\\\n",
        "{evaluation_results['f-score']:.3f}\")\n",
        "                \n",
        "    # Save                                \n",
        "    with nlp.use_params(optimizer.averages):           \n",
        "        nlp.to_disk(\"model_artifacts\")                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vupBiqpREf-y",
        "outputId": "4ecab54a-536f-40fd-e300-f2797f78f096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss\t\tPrec.\tRec.\tF-score\n",
            "16.189721\t0.862\t0.815\t0.838\n",
            " 0.174130\t0.872\t0.837\t0.854\n",
            " 0.081183\t0.879\t0.841\t0.859\n",
            " 0.073756\t0.876\t0.857\t0.866\n",
            " 0.061645\t0.876\t0.857\t0.867\n",
            " 0.053377\t0.883\t0.863\t0.873\n"
          ]
        }
      ],
      "source": [
        "train, test = load_training_data(limit=20000)\n",
        "h = train_model(train, test, iterations=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "O5edfwtQEf-z"
      },
      "outputs": [],
      "source": [
        "TEST_REVIEW = \"\"\"\n",
        "Transcendently beautiful in moments outside the office, it seems almost\n",
        "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
        "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
        "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
        "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "oftqYmFbEf-0"
      },
      "outputs": [],
      "source": [
        "def test_model(input_data: str):\n",
        "\n",
        "    loaded_model = spacy.load(\"model_artifacts\")\n",
        "    parsed_text = loaded_model(input_data)\n",
        "\n",
        "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
        "        prediction = \"+\"\n",
        "        score = parsed_text.cats[\"pos\"]\n",
        "    else:\n",
        "        prediction = \"-\"\n",
        "        score = parsed_text.cats[\"neg\"]\n",
        "    print(f\"text: {input_data}\\n\\\n",
        "Prediction: {prediction}\\n\\\n",
        "Score: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciAbdv-VEf-0",
        "outputId": "75b595cd-d4ef-4656-e594-293a74de140c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: \n",
            "Transcendently beautiful in moments outside the office, it seems almost\n",
            "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
            "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
            "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
            "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
            "\n",
            "Prediction: +\n",
            "Score: 0.992\n"
          ]
        }
      ],
      "source": [
        "test_model(input_data=TEST_REVIEW)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}